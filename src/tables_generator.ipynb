{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimenter import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm_notebook\n",
    "from tabulate import tabulate\n",
    "mcm = mixed_combinations_map\n",
    "if 'HDPa' in mcm['PA']:\n",
    "    mcm['PA'].remove('HDPa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mean_std(dfs, _round=True):\n",
    "    if _round:\n",
    "        return pd.concat(dfs).reset_index().groupby('index').\\\n",
    "              agg(lambda x: '{} $\\\\pm$ {}'.format(\"%.1f\" % round(np.mean(x), 1),\n",
    "                                               \"%.1f\" % round(np.std(x), 1)))\n",
    "    else:\n",
    "        return pd.concat(dfs).reset_index().groupby('index').\\\n",
    "              agg(lambda x: '{} $\\\\pm$ {}'.format(\"%.1f\" % np.mean(x),\n",
    "                                               \"%.1f\" % np.std(x)))\n",
    "\n",
    "def get_overfit_score_df(train_df, test_df):\n",
    "    overfit_df = train_df.subtract(test_df)\n",
    "    overfit_df = overfit_df.div(train_df)\n",
    "    overfit_df = overfit_df.mul(100)\n",
    "    return overfit_df\n",
    "\n",
    "def get_data_split_name(d, s, mode='full'):\n",
    "    if mode == 'full':\n",
    "        return '{} ({})'.format(d, s)\n",
    "    if mode == 'abbr':\n",
    "        return '{} ({})'.format(get_data_abbr(d), s[0])\n",
    "    if mode == 'idx':\n",
    "        return '{} ({})'.format(get_data_idx(d), s[0])\n",
    "\n",
    "def get_latex_table(df, file_name = None, bold_best=None, col_mode = 'math', ascending = True):\n",
    "    if col_mode == 'math':\n",
    "        table_df = df.rename(columns = {c: '${}$'.format(c) for c in df.columns})\n",
    "    elif col_mode == 'sf':\n",
    "        table_df = df.rename(columns = {c: '\\\\textsf{{{}}}'.format(c) for c in df.columns})\n",
    "    elif col_mode == 'tt':\n",
    "        table_df = df.rename(columns = {c: '\\\\texttt{{{}}}'.format(c) for c in df.columns})\n",
    "    if bold_best == 'per_col':\n",
    "        pass\n",
    "    elif bold_best == 'per_row':\n",
    "        for i in table_df.index:\n",
    "            max_i = table_df.loc[i, :].max() if ascending else table_df.loc[i, :].min()\n",
    "            table_df.loc[i, :] = table_df.loc[i, :].apply(lambda x: '\\\\textbf{{{}}}'.format(x) if x == max_i else x)\n",
    "    return table_df.to_latex(file_name, escape=False, column_format = 'l'+'c'*df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Data description\n",
    "data_description.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_names = get_data_names()\n",
    "# headers = ['Name', 'Abbr', '|V|', '|F|']\n",
    "# data_names[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) All data, macro classifiers, AUC scores: 12 x 5\n",
    "macro_feat_perf_auc.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_macro_feat_results(data_name, split_mode):\n",
    "    params = get_default_params()\n",
    "    params['data_params']['data_name'] = data_name\n",
    "    params['data_params']['split_mode'] = split_mode\n",
    "    params['data_params']['base_path'] = '/home2/e1-313-15477'\n",
    "    train_dfs, test_dfs, overfit_dfs = [], [], []\n",
    "    # i = 0\n",
    "    for i in range(5):\n",
    "        params['iter_var'] = i\n",
    "        cl_perfs = {}\n",
    "        interim_train_dfs = []\n",
    "        interim_test_dfs = []\n",
    "        G_feats = default_lp_cols\n",
    "        W_feats = ['w_{}'.format(c) for c in default_lp_cols]\n",
    "        H_feats = default_hyper_cols\n",
    "        col_map = {tuple(G_feats): 'macro-G', \n",
    "                   tuple(H_feats): 'macro-H',\n",
    "                   tuple(W_feats): 'macro-W',\n",
    "                   tuple(G_feats + H_feats): 'macro-GH',\n",
    "                   tuple(W_feats+H_feats): 'macro-WH'}\n",
    "        output = perform_GWH_classification(params, G_feats, W_feats, H_feats, 'xgboost')\n",
    "        lp_col = ''\n",
    "        train_df = pd.concat([output[k]['train_perf'].rename(columns = {'xgboost_train': k}).T for k in output]).T\n",
    "        test_df = pd.concat([output[k]['test_perf'].rename(columns = {'xgboost_test': k}).T for k in output]).T\n",
    "        train_df.rename(columns = col_map, inplace=True)\n",
    "        test_df.rename(columns = col_map, inplace=True)\n",
    "        overfit_df = get_overfit_score_df(train_df, test_df)\n",
    "\n",
    "        train_dfs.append(train_df)\n",
    "        test_dfs.append(test_df)\n",
    "        overfit_dfs.append(overfit_df)\n",
    "\n",
    "    train_df = to_mean_std(train_dfs)\n",
    "    test_df = to_mean_std(test_dfs)\n",
    "    overfit_df = to_mean_std(overfit_dfs)\n",
    "    return train_df, test_df, overfit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_names = get_data_names()\n",
    "# data_names = ['email-Enron', 'contact-high-school']\n",
    "split_modes = ['structural', 'temporal']\n",
    "train_dfs = {}\n",
    "test_dfs = {}\n",
    "overfit_dfs = {}\n",
    "iterator = list(product(data_names, split_modes))\n",
    "for d, s in tqdm_notebook(iterator):\n",
    "    train_df, test_df, overfit_df = read_macro_feat_results(d, s)\n",
    "    train_dfs[(d, s)] = train_df\n",
    "    test_dfs[(d, s)] = test_df\n",
    "    overfit_dfs[(d, s)] = overfit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'p@100'\n",
    "#'auc',\n",
    "#            'p@+',\n",
    "#            'r@+',\n",
    "#            'p@100',\n",
    "rows = []\n",
    "train_df_list = []\n",
    "for s, d in product(split_modes, data_names):\n",
    "    row = train_dfs[(d, s)].loc[metric, :]\n",
    "    row.name = get_data_split_name(d, s, 'abbr')\n",
    "    rows.append(row)\n",
    "    df1 = pd.DataFrame(row).T\n",
    "    train_df_list.append(df1)\n",
    "    \n",
    "rows = []\n",
    "test_df_list = []\n",
    "for s, d in product(split_modes, data_names):\n",
    "    row = test_dfs[(d, s)].loc[metric, :]\n",
    "    row.name = get_data_split_name(d, s, 'abbr')\n",
    "    rows.append(row)\n",
    "    df1 = pd.DataFrame(row).T\n",
    "    test_df_list.append(df1)\n",
    "\n",
    "    \n",
    "rows = []\n",
    "overfit_df_list = []\n",
    "for s, d in product(split_modes, data_names):\n",
    "    row = overfit_dfs[(d, s)].loc[metric, :]\n",
    "    row.name = get_data_split_name(d, s, 'abbr')\n",
    "    rows.append(row)\n",
    "    df1 = pd.DataFrame(row).T\n",
    "    overfit_df_list.append(df1)\n",
    "\n",
    "final_train_df = pd.concat(train_df_list)\n",
    "final_test_df = pd.concat(test_df_list)\n",
    "final_overfit_df = pd.concat(overfit_df_list)\n",
    "\n",
    "# print(tabulate(final_train_df, headers='keys', tablefmt='psql'))\n",
    "# print(tabulate(final_train_df.rank(axis=1, ascending=False), headers='keys', tablefmt='psql'))\n",
    "print(tabulate(final_test_df.rank(axis=1, ascending=False), headers='keys', tablefmt='psql'))\n",
    "print(get_latex_table(final_test_df, bold_best = 'per_row', col_mode='tt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iv) All data, micro classifiers, rank-freq scores: 12 x 5\n",
    "\n",
    "\n",
    "micro_feat_rank_perf_auc.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_preparer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (v) 1 data, micro classifiers, AUC scores: 10 x 10\n",
    "micro_feat_perf_auc_{data_name}.tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (vi) All data, standalone, rank-freq scores: 12 x 6\n",
    "standalone_rank_perf_auc.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_standalone_results(data_name, split_mode, metric):\n",
    "    params = get_default_params()\n",
    "    params['data_params']['data_name'] = data_name\n",
    "    params['data_params']['split_mode'] = split_mode\n",
    "    params['data_params']['base_path'] = '/home2/e1-313-15477'\n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        params['iter_var'] = i\n",
    "        _, lp_results = perform_link_prediction(params['data_params'],\n",
    "            params['lp_data_params'],\n",
    "            params['lp_params'],\n",
    "            params['iter_var'])\n",
    "        dfs.append(lp_results['perf'])\n",
    "    df = to_mean_std(dfs)\n",
    "\n",
    "    GWH_cols = ['stand-G', 'stand-W', 'stand-H\\\\textsubscript{max}', 'stand-H\\\\textsubscript{avg}', 'stand-H\\\\textsubscript{L1}', 'stand-H\\\\textsubscript{L2}']\n",
    "    rows = []\n",
    "    df_list = []\n",
    "    for c in default_lp_cols:\n",
    "        cols = [c, 'w_' + c] + mcm[c][1:]\n",
    "        row = df.loc[metric, cols]\n",
    "        row.name = c\n",
    "        rows.append(row)\n",
    "        df1 = pd.DataFrame(row).T\n",
    "        df1 = df1.rename(columns=dict(zip(df1.columns, GWH_cols)))\n",
    "        df_list.append(df1)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_names = get_data_names()\n",
    "data_names = ['email-Enron', 'contact-high-school', 'tags-math-sx', 'threads-math-sx','NDC-substances','coauth-DBLP']\n",
    "split_modes = ['structural', 'temporal']\n",
    "dfs = []\n",
    "iterator = list(product(split_modes, data_names))\n",
    "for s, d in tqdm_notebook(iterator):\n",
    "    df = read_standalone_results(d, s, 'r@+')\n",
    "    df = df.rank(axis=1, ascending=False)\n",
    "#     print(to_mean_std([rank_df.loc[i, :] for i in rank_df.index]))\n",
    "    df = to_mean_std([df.loc[i, :] for i in df.index]).loc[df.columns, :].rename(columns={0: get_data_split_name(d, s, mode='abbr')}).T\n",
    "    dfs.append(df)\n",
    "table_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(table_df.rank(axis=1, ascending=True), headers='keys', tablefmt='psql'))\n",
    "print(get_latex_table(table_df, bold_best = 'per_row', col_mode='tt', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (vii) 1 data, standalone, AUC scores: 10 x 12\n",
    "standalone_perf_auc_{data_name}.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'coauth-DBLP'\n",
    "df1 = read_standalone_results(data_name, 'structural', 'auc')\n",
    "df2 = read_standalone_results(data_name, 'temporal', 'auc')\n",
    "df = pd.concat([df1.rename(index={c: '{} ({})'.format(c, 's') for c in df1.index}),\n",
    "           df2.rename(index={c: '{} ({})'.format(c, 't') for c in df2.index})])\n",
    "print(get_latex_table(df, bold_best = 'per_row', col_mode='tt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (viii) All data, all metrics, best performing (performance): 12 x 4\n",
    "metric_best_performing.tex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:genv] *",
   "language": "python",
   "name": "conda-env-genv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
