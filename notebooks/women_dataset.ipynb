{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.utils import get_data_abbr, mkdir_p, get_base_path, get_library_path\n",
    "import pandas as pd\n",
    "# from scipy.stats import kendalltau\n",
    "import numpy as np\n",
    "\n",
    "library_path = get_library_path()\n",
    "sys.path.append(library_path)\n",
    "sys.path.append(os.path.join(library_path, \"hynetworkx\"))\n",
    "\n",
    "from src.data_preparer import filter_size, prepare_lp_data, get_time_filter_params\n",
    "from src.hypergraph_link_predictor import get_hypergraph_scores, hypergraph_score_abbr_map, all_hypergraph_score_names\n",
    "from src.link_predictor import get_perf_df\n",
    "from src.linkpred_predictor import get_linkpred_scores, predictor_abbr_map, all_predictor_names\n",
    "from src.supervised_link_predictor import classify\n",
    "from src.experimenter import perform_GWH_classification\n",
    "from src.incidence_matrix import parse_benson_incidence_matrix as parse_S\n",
    "#from scr.tables_generator.ipynb import to_mean_std,get_overfit_score_df,get_data_split_name,get_latex_table,read_macro_feat_results,read_standalone_results\n",
    "\n",
    "from joblib import Memory\n",
    "\n",
    "base_path = get_base_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/content/gdrive/My Drive/Colab Notebooks/data/'\n",
    "data_params = {'data_name': 'contact-high-school',\n",
    "               'base_path': base_path,\n",
    "               'split_mode': 'structural',\n",
    "               'max_size_limit': 10}\n",
    "lp_data_params = {'rho': 0.2,\n",
    "                  'neg_factor': 5,\n",
    "                  'neg_mode': 'random',\n",
    "                  'weighted_flag': False}\n",
    "lp_params = {'linkpred_indices': None,  # Say None for all scores\n",
    "             'hypergraph_score_indices': None,  # Say None for all scores\n",
    "             }\n",
    "classifier_params = {'features': None,\n",
    "                     'classifier': 'xgboost'}\n",
    "column_s = ['classifier']\n",
    "data_names = ['women']\n",
    "split_modes = ['structural']\n",
    "weighted_flags = [False, True]\n",
    "\n",
    "lp_cols = ['AA', 'AS', 'CN', 'Cos', 'PA', 'JC', 'MxO', 'MnO', 'NM', 'Prn']\n",
    "metrics = ['ap', 'auc', 'p@+', 'p@10', 'p@100', 'r@+', 'r@10', 'r@100']\n",
    "hyper_cols = ['HAAM', 'HAAa', 'HAAl1', 'HAAl2',\n",
    "              'HASM', 'HASa', 'HASl1', 'HASl2',\n",
    "              'HCNM', 'HCNa', 'HCNl1', 'HCNl2',\n",
    "              'HCosM', 'HCosa', 'HCosl1', 'HCosl2',\n",
    "              'HDPa', 'HPM', 'HPa', 'HPl1', 'HPl2',\n",
    "              'HJCM', 'HJCa', 'HJCl1', 'HJCl2',\n",
    "              'HmaxoM', 'HmaxoA', 'Hmaxol1', 'Hmaxol2',\n",
    "              'HminoM', 'HminoA', 'Hminol1', 'Hminol2',\n",
    "              'HNMM', 'HNMa', 'HNMl1', 'HNMl2',\n",
    "              'HPearM', 'HPeara', 'HPearl1', 'HPearl2',\n",
    "              ]\n",
    "\n",
    "combined_tables = {}\n",
    "mixed_combinations_map = {'AA': ['AA', 'HAAM', 'HAAa', 'HAAl1', 'HAAl2'],\n",
    "                          'AS': ['AS', 'HASM', 'HASa', 'HASl1', 'HASl2'],\n",
    "                          'CN': ['CN', 'HCNM', 'HCNa', 'HCNl1', 'HCNl2'],\n",
    "                          'Cos': ['Cos', 'HCosM', 'HCosa', 'HCosl1', 'HCosl2'],\n",
    "                          'PA': ['PA', 'HDPa', 'HPM', 'HPa', 'HPl1', 'HPl2'],\n",
    "                          'JC': ['JC', 'HJCM', 'HJCa', 'HJCl1', 'HJCl2'],\n",
    "                          'Kz': ['Kz', 'HKz'],\n",
    "                          'MxO': ['MxO', 'HmaxoM', 'HmaxoA', 'Hmaxol1', 'Hmaxol2'],\n",
    "                          'MnO': ['MnO', 'HminoM', 'HminoA', 'Hminol1', 'Hminol2'],\n",
    "                          'NM': ['NM', 'HNMM', 'HNMa', 'HNMl1', 'HNMl2'],\n",
    "                          'Prn': ['Prn', 'HPearM', 'HPeara', 'HPearl1', 'HPearl2'], }\n",
    "\n",
    "\n",
    "data_params={'data_name': 'email-Enron',\n",
    "                                      'base_path': base_path,\n",
    "                                      'split_mode': 'temporal',\n",
    "                                      'max_size_limit': 10}\n",
    "default_lp_cols = ['AA', 'AS', 'CN', 'Cos', 'PA', 'JC', 'MxO', 'MnO', 'NM', 'Prn']\n",
    "abbr_pred_map = {a: p for p, a in predictor_abbr_map.items()}\n",
    "default_lp_names = [abbr_pred_map[a] for a in default_lp_cols]\n",
    "\n",
    "lp_data_params={'rho': 0.2,\n",
    "                                         'neg_factor': 5,\n",
    "                                         'neg_mode': 'random'}\n",
    "default_lp_indices = [all_predictor_names.index(p) for p in default_lp_names]\n",
    "default_hyper_cols = ['HAAM', 'HAAa', 'HAAl1', 'HAAl2',\n",
    "                      'HASM', 'HASa', 'HASl1', 'HASl2',\n",
    "                      'HCNM', 'HCNa', 'HCNl1', 'HCNl2',\n",
    "                      'HCosM', 'HCosa', 'HCosl1', 'HCosl2',\n",
    "                      'HDPa', 'HPM', 'HPa', 'HPl1', 'HPl2',\n",
    "                      'HJCM', 'HJCa', 'HJCl1', 'HJCl2',\n",
    "                      'HmaxoM', 'HmaxoA', 'Hmaxol1', 'Hmaxol2',\n",
    "                      'HminoM', 'HminoA', 'Hminol1', 'Hminol2',\n",
    "                      'HNMM', 'HNMa', 'HNMl1', 'HNMl2',\n",
    "                      'HPearM', 'HPeara', 'HPearl1', 'HPearl2',\n",
    "                      ]\n",
    "hyg_abbr_pred_map = {a: p for p, a in hypergraph_score_abbr_map.items()}\n",
    "default_hyg_names = [hyg_abbr_pred_map[a] for a in default_hyper_cols]\n",
    "default_hyper_indices = [all_hypergraph_score_names.index(p) for p in default_hyg_names]\n",
    "\n",
    "\n",
    "lp_params={'linkpred_indices': default_lp_indices,\n",
    "                                    'hypergraph_score_indices': default_hyper_indices}\n",
    "iter_var= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, triu, hstack, find\n",
    "\n",
    "def parse_s_women():\n",
    "    df=pd.read_csv(\"../../out.opsahl-southernwomen\",header=None, sep=' ')\n",
    "    df=df[[0,1]]\n",
    "    number_of_women=max(df[0])\n",
    "    number_of_functions=max(df[1])\n",
    "    hyperedges = set()\n",
    "    hyperedge_list = []\n",
    "    hyperedge_times=[]\n",
    "    rows = []\n",
    "    cols = []\n",
    "    j = 0\n",
    "    i = 0\n",
    "    hyperedge_women_map = defaultdict(list)\n",
    "    for k in range (len(df[1])):\n",
    "        hyperedge_women_map[df[1][k]].append(df[0][k])\n",
    "\n",
    "    for k in range (1,number_of_functions+1):\n",
    "        hyperedge = frozenset(hyperedge_women_map[k])\n",
    "\n",
    "    #     hyperedge_list.append(hyperedge)\n",
    "        if hyperedge not in hyperedges:\n",
    "            hyperedges.add(hyperedge)\n",
    "            #print([x-1 for x in hyperedge])\n",
    "            rows.extend([x-1 for x in hyperedge])\n",
    "            cols.extend([j] * len(hyperedge))\n",
    "            j=j+1\n",
    "            hyperedge_times.append(0)\n",
    "\n",
    "    m = len(hyperedges)\n",
    "    n = number_of_women\n",
    "    matrix = csr_matrix(([1] * len(rows), (rows, cols)), shape=(n, m))\n",
    "#     id_label_map = {v.id: v.label for v in vertex_list}\n",
    "    return matrix, np.array(hyperedge_times)\n",
    "\n",
    "default_lp_cols = ['AA', 'AS', 'CN', 'Cos', 'PA', 'JC', 'MxO', 'MnO', 'NM', 'Prn']\n",
    "abbr_pred_map = {a: p for p, a in predictor_abbr_map.items()}\n",
    "default_lp_names = [abbr_pred_map[a] for a in default_lp_cols]\n",
    "default_lp_indices = [all_predictor_names.index(p) for p in default_lp_names]\n",
    "\n",
    "metrics = ['ap', 'auc', 'p@+', 'p@10', 'p@100', 'r@+', 'r@10', 'r@100']\n",
    "default_hyper_cols = ['HAAM', 'HAAa', 'HAAl1', 'HAAl2',\n",
    "                      'HASM', 'HASa', 'HASl1', 'HASl2',\n",
    "                      'HCNM', 'HCNa', 'HCNl1', 'HCNl2',\n",
    "                      'HCosM', 'HCosa', 'HCosl1', 'HCosl2',\n",
    "                      'HDPa', 'HPM', 'HPa', 'HPl1', 'HPl2',\n",
    "                      'HJCM', 'HJCa', 'HJCl1', 'HJCl2',\n",
    "                      'HmaxoM', 'HmaxoA', 'Hmaxol1', 'Hmaxol2',\n",
    "                      'HminoM', 'HminoA', 'Hminol1', 'Hminol2',\n",
    "                      'HNMM', 'HNMa', 'HNMl1', 'HNMl2',\n",
    "                      'HPearM', 'HPeara', 'HPearl1', 'HPearl2',\n",
    "                      ]\n",
    "hyg_abbr_pred_map = {a: p for p, a in hypergraph_score_abbr_map.items()}\n",
    "default_hyg_names = [hyg_abbr_pred_map[a] for a in default_hyper_cols]\n",
    "default_hyper_indices = [all_hypergraph_score_names.index(p) for p in default_hyg_names]\n",
    "\n",
    "combined_tables = {}\n",
    "mixed_combinations_map = {'AA': ['AA', 'HAAM', 'HAAa', 'HAAl1', 'HAAl2'],\n",
    "                          'AS': ['AS', 'HASM', 'HASa', 'HASl1', 'HASl2'],\n",
    "                          'CN': ['CN', 'HCNM', 'HCNa', 'HCNl1', 'HCNl2'],\n",
    "                          'Cos': ['Cos', 'HCosM', 'HCosa', 'HCosl1', 'HCosl2'],\n",
    "                          'PA': ['PA', 'HDPa', 'HPM', 'HPa', 'HPl1', 'HPl2'],\n",
    "                          'JC': ['JC', 'HJCM', 'HJCa', 'HJCl1', 'HJCl2'],\n",
    "                          'Kz': ['Kz', 'HKz'],\n",
    "                          'MxO': ['MxO', 'HmaxoM', 'HmaxoA', 'Hmaxol1', 'Hmaxol2'],\n",
    "                          'MnO': ['MnO', 'HminoM', 'HminoA', 'Hminol1', 'Hminol2'],\n",
    "                          'NM': ['NM', 'HNMM', 'HNMa', 'HNMl1', 'HNMl2'],\n",
    "                          'Prn': ['Prn', 'HPearM', 'HPeara', 'HPearl1', 'HPearl2'], }\n",
    "\n",
    "\n",
    "def perform_link_prediction1(data_params, lp_data_params, lp_params=None, iter_var=0):\n",
    "    \"\"\"\n",
    "    data_params: {'data_name', 'base_path', 'split_mode', 'max_size_limit'}\n",
    "    lp_data_params: {'rho', 'neg_factor', 'neg_mode', 'weighted_flag'}\n",
    "    lp_params: {'linkpred_indices', 'hypergraph_score_indices'}\n",
    "\n",
    "    returns: (data, lp_data, lp_results)\n",
    "    \"\"\"\n",
    "    #print('READING DATASET...')\n",
    "    data_name, base_path, split_mode, max_size_limit = [data_params[x] for x in\n",
    "                                                        ['data_name', 'base_path', 'split_mode', 'max_size_limit']]\n",
    "\n",
    "    #print('PREPARING LP DATA...')\n",
    "    rho, neg_factor, neg_mode = [lp_data_params[x] for x in\n",
    "                                 ['rho', 'neg_factor', 'neg_mode']]\n",
    "\n",
    "    S, times = parse_s_women()\n",
    "    weighted_lp_data = prepare_lp_data(S, True, times, rho, neg_factor, neg_mode)\n",
    "    #print(weighted_lp_data)\n",
    "\n",
    "    #print('PERFORMING LINK PREDICTION...')\n",
    "    if lp_params:\n",
    "        linkpred_indices, hypergraph_score_indices = [lp_params[x] for x in\n",
    "                                                      ['linkpred_indices', 'hypergraph_score_indices']]\n",
    "    else:\n",
    "        linkpred_indices, hypergraph_score_indices = None, None\n",
    "        \n",
    "        \n",
    "            \n",
    "    print(linkpred_indices)\n",
    "    print(hypergraph_score_indices)\n",
    "        \n",
    "    weighted_linkpred_scores_df = get_linkpred_scores(weighted_lp_data, True, linkpred_indices)\n",
    "    unweighted_linkpred_scores_df = get_linkpred_scores(weighted_lp_data, False, linkpred_indices)\n",
    "    unweighted_linkpred_cols = list(unweighted_linkpred_scores_df.columns)\n",
    "    cols_map = {c: 'w_{}'.format(c) for c in unweighted_linkpred_cols}\n",
    "    weighted_linkpred_scores_df = weighted_linkpred_scores_df.rename(columns=cols_map)\n",
    "    weighted_linkpred_cols = list(weighted_linkpred_scores_df.columns)\n",
    "\n",
    "    hyg_scores_df = get_hypergraph_scores(weighted_lp_data, hypergraph_score_indices)\n",
    "    hyg_scores_cols = list(hyg_scores_df.columns)\n",
    "    scores_df = pd.merge(unweighted_linkpred_scores_df, weighted_linkpred_scores_df, left_index=True, right_index=True)\n",
    "    scores_df = pd.merge(scores_df, hyg_scores_df, left_index=True, right_index=True)\n",
    "    pos_pairs = set(zip(*weighted_lp_data['A_test_pos'].nonzero()))\n",
    "    scores_df['label'] = scores_df.index.map(lambda x: int(x in pos_pairs))\n",
    "    perf_df = get_perf_df(scores_df, unweighted_linkpred_cols + weighted_linkpred_cols, hyg_scores_cols)\n",
    "    return weighted_lp_data, \\\n",
    "           {'scores': scores_df, 'perf': perf_df}\n",
    "\n",
    "\n",
    "\n",
    "a,b=perform_link_prediction1(data_params, lp_data_params, lp_params=lp_params, iter_var=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c=b['perf']\n",
    "# c[['AA','w_AA', 'HAAM', 'HAAa', 'HAAl1', 'HAAl2']].rank(axis=1, ascending=False).T.sort_values('p@+', ascending=True).head(20)\n",
    "c[['AA', 'w_AA','HAAM', 'HAAa', 'HAAl1', 'HAAl2']].T.sort_values('p@+', ascending=True).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=b['scores']\n",
    "d=c[['AA','w_AA', 'HAAM', 'HAAa', 'HAAl1', 'HAAl2','label']]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.rank(axis=0, ascending=False)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_standalone_results(data_name, split_mode, metric):\n",
    "    #params = get_default_params()\n",
    "    data_params['data_name'] = data_name\n",
    "    data_params['split_mode'] = split_mode\n",
    "    data_params['base_path'] = '/home2/e1-313-15477'\n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        iter_var= i\n",
    "        _, lp_results = perform_link_prediction1(data_params,\n",
    "            lp_data_params,\n",
    "            lp_params,\n",
    "            iter_var)\n",
    "        dfs.append(lp_results['perf'])\n",
    "    df = to_mean_std(dfs)\n",
    "\n",
    "    GWH_cols = ['stand-G', 'stand-W', 'stand-H\\\\textsubscript{max}', 'stand-H\\\\textsubscript{avg}', 'stand-H\\\\textsubscript{L1}', 'stand-H\\\\textsubscript{L2}']\n",
    "    rows = []\n",
    "    df_list = []\n",
    "    for c in default_lp_cols:\n",
    "        cols = [c, 'w_' + c] + mcm[c][1:]\n",
    "        row = df.loc[metric, cols]\n",
    "        row.name = c\n",
    "        rows.append(row)\n",
    "        df1 = pd.DataFrame(row).T\n",
    "        df1 = df1.rename(columns=dict(zip(df1.columns, GWH_cols)))\n",
    "        df_list.append(df1)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mean_std(dfs, _round=True):\n",
    "    if _round:\n",
    "        return pd.concat(dfs).reset_index().groupby('index').\\\n",
    "              agg(lambda x: '{} $\\\\pm$ {}'.format(\"%.1f\" % round(np.mean(x), 1),\n",
    "                                               \"%.1f\" % round(np.std(x), 1)))\n",
    "    else:\n",
    "        return pd.concat(dfs).reset_index().groupby('index').\\\n",
    "              agg(lambda x: '{} $\\\\pm$ {}'.format(\"%.1f\" % np.mean(x),\n",
    "                                               \"%.1f\" % np.std(x)))\n",
    "    \n",
    "def get_data_split_name(d, s, mode='full'):\n",
    "    if mode == 'full':\n",
    "        return '{} ({})'.format(d, s)\n",
    "    if mode == 'abbr':\n",
    "        return 'wom'\n",
    "    if mode == 'idx':\n",
    "        return '{} ({})'.format(get_data_idx(d), s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm_notebook\n",
    "from tabulate import tabulate\n",
    "mcm = mixed_combinations_map\n",
    "if 'HDPa' in mcm['PA']:\n",
    "    mcm['PA'].remove('HDPa')\n",
    "data_names = ['women']\n",
    "split_modes = ['structural']\n",
    "dfs = []\n",
    "iterator = list(product(split_modes, data_names))\n",
    "for s, d in tqdm_notebook(iterator):\n",
    "    df = read_standalone_results(d, s, 'auc')\n",
    "    df = df.rank(axis=1, ascending=False)\n",
    "#     print(to_mean_std([rank_df.loc[i, :] for i in rank_df.index]))\n",
    "    df = to_mean_std([df.loc[i, :] for i in df.index]).loc[df.columns, :].rename(columns={0: get_data_split_name(d, s, mode='abbr')}).T\n",
    "    dfs.append(df)\n",
    "table_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(table_df.rank(axis=1, ascending=True), headers='keys', tablefmt='psql'))\n",
    "#print(get_latex_table(table_df, bold_best = 'per_row', col_mode='tt', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparer import S_to_A, S_to_B, incidence_to_hyperedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, times = parse_s_women()\n",
    "S.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['S_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incidence_to_hyperedges(a['S_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:genv] *",
   "language": "python",
   "name": "conda-env-genv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
